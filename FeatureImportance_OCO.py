# -*- coding: utf-8 -*-
"""superresolution_colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11WIESvAMAUbJVItrAGnVNn1j_EityrEg
"""


#general packages
import os
from os import listdir
from os.path import isfile, join
import csv
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#import seaborn as sns
from PIL import Image
import netCDF4 
import random
import sys

#PyTorch
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tqdm.notebook import tqdm
import torch.optim as optim
from torch.utils.data.dataset import Subset
from torch.nn import functional as F
#import torchvision

#other ML packags
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold
from scipy.stats import pearsonr
from skimage.metrics import structural_similarity as ssim
import optuna

import pytorch_ssim

# Module for Google Drive
#from google.colab import drive
#mount drive
#from google.colab import drive
#drive.mount('/content/drive')

#some settings
plt.rcParams.update({'font.size':14})

#own scripts
import sys
#sys.path.insert(1, '../')
import helpers
from dataset_OCO import get_500m_dataset
from dataset_OCO import DataSet
from ModelClass import SR_SIF



print('All imports worked!')
"""Settings"""

#specify your device ('cuda' or 'cpu')
device = 'cuda'

#randomness settings to deterministic for this run
#np.random.seed(1)
#torch.manual_seed(1)
#torch.use_deterministic_algorithms(True)
#if torch.cuda.is_available():
#    torch.cuda.manual_seed_all(1)

"""Dataset"""


def RunSet(model, dataloader):
  sample = next(iter(dataloader))['sif'].detach().numpy()
  print('sample.shape',sample.shape)
  pred = np.zeros((len(dataloader), sample.shape[1], sample.shape[2]))
  targ = np.zeros((len(dataloader), sample.shape[1], sample.shape[2]))

  i = 0
  sample = next(iter(dataloader))
  model.eval()
  for sample in dataloader:
    with torch.no_grad():
        inp = sample['features'].to(device, dtype=torch.float) #sample['features'].to(device, dtype=torch.float)
        target = sample['sif'].to(device)
        #print('type(inp)',type(inp))
        inp = inp.permute(0,3,1,2)
        pr = model(inp)
        pr = pr.permute(0,2,3,1)
        pr = pr.squeeze(-1)        
        pred[i,::] = pr.cpu().detach().numpy()[0,0,::]
        targ[i,::] = target.cpu().detach().numpy()[0,0,::]
        i+=1

  return targ, pred


# load a model checkpoint
# Code referred from: https://discuss.pytorch.org/t/saving-customized-model-architecture/21512/2
def load_checkpoint(filepath):
    if torch.cuda.is_available() == False or device == 'cpu':
        checkpoint = torch.load(filepath, map_location=torch.device('cpu')) 
    else:
        checkpoint = torch.load(filepath)
    model = nn.DataParallel(checkpoint['model'])
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)

    return model

"""# Find best model structure based on one sample

# Cross Validate
"""
def FeatureImportance(inp_te, dict_oco_te, keys, dates_te, days_since_test, lon, lat, model, pathPref, results_folder, BATCH_SIZE_TEST=1, FI_RUNS=2, DATA_OPTION=1, SaveFileName='Results_FeatureImportances.nc'):
  
  comp_clusters = True
  if DATA_OPTION==1 or DATA_OPTION==19:
    cluster_keys = ['MODIS_VIs', 'MODIS_VIwithoutNIRv', 'MODIS_bands', 'meteo', 'SM', 'LULC']
    clusters_dyn = {'MODIS_VIs': ['NIRv', 'kNDVI','NDVI','EVI'],
                'MODIS_VIwithoutNIRv': ['kNDVI','NDVI','EVI'],
                'MODIS_bands': ['NIR', 'RED', 'Blue', 'Green', 'SWIR1', 'SWIR2','SWIR3'],
                'meteo': ['temperature', 'precipitation', 'temperature_delay', 'precipitation_delay'],
                'SM': ['ssm', 'susm'],
                'LULC': ['LULC_Others', 'LULC_ENF', 'LULC_EBF', 'LULC_DNF', 'LULC_DBF', 'LULC_Mixed Forest', 'LULC_Unknown Forest', 'LULC_Shrubland', 'LULC_Grassland', 'LULC_Cropland', 'LULC_Wetland']
                }
    #clusters_stat = {,}
    
  elif DATA_OPTION==13:
    cluster_keys = ['MODIS_bands', 'meteo', 'SM', 'LULC']
    clusters_dyn = {'MODIS_bands': ['NIR', 'RED', 'Blue', 'Green', 'SWIR1', 'SWIR2','SWIR3'],
                'meteo': ['temperature', 'precipitation', 'temperature_delay', 'precipitation_delay'],
                'SM': ['ssm', 'susm']
                }
    clusters_stat = {'LULC': ['LULC_Others', 'LULC_ENF', 'LULC_EBF', 'LULC_DNF', 'LULC_DBF', 'LULC_Mixed Forest', 'LULC_Unknown Forest', 'LULC_Shrubland', 'LULC_Grassland', 'LULC_Cropland', 'LULC_Wetland'],
                    'fragmentation': ['forest_share', 'edge_share']
                    }
  elif DATA_OPTION==15 or DATA_OPTION==20:
    cluster_keys = ['MODIS_bands', 'meteo', 'SM', 'LULC']
    clusters_dyn = {'MODIS_bands': ['NIR', 'RED', 'Blue', 'Green', 'SWIR1', 'SWIR2','SWIR3'],
                'meteo': ['temperature', 'precipitation', 'temperature_delay', 'precipitation_delay'],
                'SM': ['ssm', 'susm']
                }
    clusters_stat = {'LULC': ['LULC_Others', 'LULC_ENF', 'LULC_EBF', 'LULC_DNF', 'LULC_DBF', 'LULC_Mixed Forest', 'LULC_Unknown Forest', 'LULC_Shrubland', 'LULC_Grassland', 'LULC_Cropland', 'LULC_Wetland'],
                    'fragmentation': ['forest_share', 'edge_share']
                    }
  else:
    cluster_keys = []
    clusters_dyn = {}
    #clusters_stat = {}
    comp_clusters = False
  
  if comp_clusters:
    clusters_dyn_ind = {}
    for k in clusters_dyn.keys():
      liste = clusters_dyn[k]
      liste_ind = []
      for var in liste:
        liste_ind.append(keys.index(var))
      clusters_dyn_ind[k] = liste_ind
    '''

    keys_stat_start = keys.index('land_mask')
    keys_stat = keys[keys_stat_start:]
    clusters_stat_ind = {}
    for k in clusters_stat.keys():
      liste = clusters_stat[k]
      liste_ind = []
      for var in liste:
        liste_ind.append(keys_stat.index(var))
      clusters_stat_ind[k] = liste_ind
    '''

  df = pd.DataFrame({}, index = keys)

  for loopFI in range(FI_RUNS):
    pred_test = {}

    feat_dim = inp_te.shape[3]

    dataset_test = DataSet(inp_te, dict_oco_te) #
    dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE_TEST, shuffle=False)
    targ_test, pred_test_nochange = RunSet(model, dataloader_test)

    #timely changing maps
    for key_flip_ind in range(feat_dim):
      feat_te_sh = inp_te.copy()
      feat_shuffle = feat_te_sh[:,:,:,key_flip_ind].copy()
      sh_shuffle = feat_shuffle.shape
      x = feat_shuffle.flatten()
      np.random.shuffle(x)
      feat_te_sh[:,:,:,key_flip_ind] = x.reshape(sh_shuffle)

      dataset_test = DataSet(feat_te_sh, dict_oco_te) #
      dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)

      targ_test, pred_test[key_flip_ind] = RunSet(model, dataloader_test)

    key_flip_ind += 1
    '''
    #static maps
    for key_flip_ind_static in range(feat_static_te.shape[2]):
      feat_static_te_sh = feat_static_te.copy()
      feat_shuffle = feat_static_te_sh[:,:,key_flip_ind_static].copy()
      sh_shuffle = feat_shuffle.shape
      x = feat_shuffle.flatten()
      np.random.shuffle(x)
      feat_static_te_sh[:,:,key_flip_ind_static] = x.reshape(sh_shuffle)

      dataset_test = DataSet(feat_te_sh,feat_static_te_sh,sif_te, CV_FLAG_MULTIREGION=False, TrainFlag=False,key_flip_ind=-1) #
      dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)

      targ_test, pred_test[key_flip_ind] = RunSet(model, dataloader_test)
      key_flip_ind += 1
    '''

    ###################################
    #Run Clusters
    if comp_clusters:
      pred_test_clusters = {}

      #clusters dynamic
      for k,loopClusters in zip(clusters_dyn.keys(),range(len(clusters_dyn.keys()))):
        
        feat_te_sh = inp_te.copy()

        #run current cluster  
        indices = clusters_dyn_ind[k]
        for loopKey in indices:
          feat_shuffle = feat_te_sh[:,:,:,loopKey].copy()
          sh_shuffle = feat_shuffle.shape
          x = feat_shuffle.flatten()
          np.random.shuffle(x)
          feat_te_sh[:,:,:,loopKey] = x.reshape(sh_shuffle)

        dataset_test = DataSet(feat_te_sh, dict_oco_te) #
        dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)

        targ_test, pred_test_clusters[k] = RunSet(model, dataloader_test)
      '''
      #clusters static
      for k,loopClusters in zip(clusters_stat.keys(),range(len(clusters_stat.keys()))):
        
        feat_static_te_sh = feat_static_te.copy()

        #run current cluster
        indices = clusters_stat_ind[k]
        for loopKey in indices:
          feat_shuffle = feat_static_te_sh[:,:,loopKey].copy()
          sh_shuffle = feat_shuffle.shape
          x = feat_shuffle.flatten()
          np.random.shuffle(x)
          feat_static_te_sh[:,:,loopKey] = x.reshape(sh_shuffle)

        dataset_test = DataSet(feat_te_sh, sif_te, CV_FLAG_MULTIREGION=False, TrainFlag=False,key_flip_ind=-1) #
        dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)

        targ_test, pred_test_clusters[k] = RunSet(model, dataloader_test)
      '''

    #FIs calculated for individual features 
    rmse_FI = []
    r2_FI = []
    gt = targ_test.flatten()
    for i in range(len(pred_test)):
        x_FI = pred_test[i].flatten()
        rmse_FI.append(np.sqrt(mean_squared_error(gt, x_FI)))
        r2_FI.append(r2_score(gt, x_FI))

    rmse_FI_noChange = np.sqrt(mean_squared_error(gt, pred_test_nochange.flatten()))
    r2_FI_noChange = r2_score(gt, pred_test_nochange.flatten())
    
    rmse_FI_diff = (rmse_FI - rmse_FI_noChange)
    r2_FI_diff = (r2_FI_noChange - r2_FI)
    df['rmse_FI_diffDivSum_run'+str(loopFI)] = rmse_FI_diff / np.sum(rmse_FI_diff) * 100
    df['r2_FI_diff_run'+str(loopFI)] = r2_FI_diff / np.sum(r2_FI_diff) * 100

    df['rmse_FI_diff_run'+str(loopFI)] = rmse_FI_diff
    df['r2_FI_diff_run'+str(loopFI)] = r2_FI_diff

    df['rmse_FI_share_run'+str(loopFI)] = rmse_FI / rmse_FI_noChange
    df['r2_FI_share_run'+str(loopFI)] = r2_FI / r2_FI_noChange


  df.to_csv(pathPref +results_folder+ SaveFileName.replace('.nc','.csv'))
  print('##############################################')
  print('Feature Importances: diff = err_FI - err_FI_noChange and share = rmse_FI / rmse_FI_noChange')
  print(df)
  print('##############################################')

  '''
  print('Saving feature importances of run ', FI_RUNS)
  print('targ_test.shape',targ_test.shape)
  print('lon.shape',lon.shape)
  print('lat.shape',lat.shape)
  
  OutputFile = netCDF4.Dataset(pathPref+results_folder+SaveFileName, "w", format="NETCDF4")

  len1 = OutputFile.createDimension("lon", targ_test.shape[2])
  len2 = OutputFile.createDimension("lat", targ_test.shape[1])
  len3 = OutputFile.createDimension("time", targ_test.shape[0])

  var_lon = OutputFile.createVariable("lon", "f4", ("lon",))
  var_lat = OutputFile.createVariable("lat", "f4", ("lat",))
  var_dates = OutputFile.createVariable("time", "f4", ("time",))
  var_sif_gt = OutputFile.createVariable("sif_gt", "f4", ("time","lat","lon"))
  var_sif_noShuffle = OutputFile.createVariable("sif_pred_noShuffle", "f4", ("time","lat","lon"))
  var_sif_lr = OutputFile.createVariable("sif_lr", "f4", ("time","lat","lon"))

  var_dates.units = 'days since 1970-01-01'
  var_lon[:] = range(4)
  var_lat[:] = range(5)
  var_sif_gt[:] = targ_test
  var_sif_lr[:] = inp_te[:,:,:,0]
  var_dates[:] = days_since_test
  var_sif_noShuffle[:] = pred_test_nochange
  
  print('########################################################################################')
  print('Saving Feature Importances!')
  print('indidual variables')
  
  #individual variables
  i=0
  for k in pred_test:
    #print(keys[i])
    var_sif_pred = OutputFile.createVariable("sif_pred_"+keys[i], "f4", ("time","lat","lon"))
    var_sif_pred[:] = pred_test[i]
    i+=1

  if comp_clusters:
    print('clusters')
    #clusters
    for k in pred_test_clusters.keys():
      #print(k)
      var_sif_pred = OutputFile.createVariable("sif_pred_cluster_"+k, "f4", ("time","lat","lon"))
      var_sif_pred[:] = pred_test_clusters[k]

  OutputFile.close()
  '''
  #df.to_csv(pathPref+results_folder+SaveFileName.replace('.nc','.csv'))


