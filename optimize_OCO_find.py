# -*- coding: utf-8 -*-
"""superresolution_colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11WIESvAMAUbJVItrAGnVNn1j_EityrEg
"""


#general packages
import os
from os import listdir
from os.path import isfile, join
import csv
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#import seaborn as sns
from PIL import Image
import netCDF4 
import h5py
import random
import sys
from multiprocessing import Manager
import multiprocessing
from contextlib import contextmanager

#PyTorch
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from tqdm.notebook import tqdm
import torch.optim as optim
from torch.utils.data.dataset import Subset
from torch.nn import functional as F
#import torchvision

#other ML packags
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold
from scipy.stats import pearsonr
#from skimage.metrics import structural_similarity as ssim
import optuna
import sys



################################################################################################################
#own scripts
#sys.path.insert(1, '../')
import pytorch_ssim
import helpers
from dataset_OCO import get_500m_dataset
from dataset_OCO import DataSet
from ModelClass5_4 import SR_SIF
from FeatureImportance_OCO import FeatureImportance
from Calc500mEstimate_OCO import Calc500m
#from validate2OCO import Comp2OCO2

################################################################################################################
#some settings
#randomness settings to deterministic for this run
np.random.seed(1)
torch.manual_seed(1)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(1)

plt.rcParams.update({'font.size':14})

################################################################################################################
print('All imports worked!')
"""Settings"""

########################################################
#specify your device ('cuda' or 'cpu')
device = 'cuda'

################################################################################################################
############## OPTIONS
BATCH_SIZE = 200
BATCH_SIZE_TEST = 1#torch.cuda.device_count()
MIN_EPOCHS = 5
EPOCHS = 50
CV_FLAG = False
SCALE_FACTOR = 10
pathPref = '/mnt/mnt_folder/results/'
modelName_woTestSet = '/OptmizedModel_woTestSet_scaling'+str(SCALE_FACTOR)+'.pth'
modelName_withTestSet = '/OptmizedModel_withTestSet_scaling'+str(SCALE_FACTOR)+'.pth'
STEPS_TIME = 1
STEPS_TIME_VAL = 1
TRAIN_DATA_AUG_FLAG = True
VAL_TEST_DATA_AUG_FLAG  = False
ENQUEUE_TRIAL = True
SAMPLING_SIDE_LEN = 100
SAMPLING_SIDE_LEN_VAL = 100
FI_EPOCH = False
CALC_TESTLOSS_EPOCH = True

#MODEL OPTION -> needs to be changed in ModelClass.py
MOD = 5

#data_option
DATA_OPTION = 1
# 1: <reduced memory> ['LR_SIF', 'NIRv', 'kNDVI', 'EVI', 'NDVI', 'NIR', 'RED', 'Blue', 'Green', 'SWIR1', 'SWIR2','SWIR3', 'cosSZA', 'temperature', 'precipitation', 'temperature_delay', 'precipitation_delay', 'ssm', 'susm', 'land_mask', 'topography', 'forest_share', 'edge_share', <all LULC>]

results_folder = 'DO'+str(DATA_OPTION)+'_MOD0'+str(MOD) + '.4_comp2OCO_DLcorrected_train_E5-20_N40-55_test_E5-15_N45-50_small_find/'
print('results_folder: ', results_folder)


NUMBER_TRIALS = 5
USE_BEST_PARAMS = True
if USE_BEST_PARAMS:
  #best_params = {'learning_rate': 0.00019389907838528278, 'weight_decay': 0.0003402671276343148, 'max_epochs': 38} #DO11
  #best_params = {'learning_rate': 0.0002634252699379683, 'weight_decay': 2.0951290983041437e-05, 'max_epochs': 32} #DO12 MORE CHANNELS MODEL Trial 252
  #best_params = {'learning_rate': 0.0018407575580173478, 'weight_decay': 5.699058926927614e-05, 'max_epochs': 42} #DO12 MOD05 Trial 244 best par of former model
  #best_params = {'learning_rate': 0.0007086284808277589, 'weight_decay': 0.0006397916764439701} #DO1
  #best_params = {'learning_rate': 0.0011003247564052153, 'weight_decay': 0.024838778860236294} #DO7
  best_params = {'learning_rate': 0.0000341467159499725, 'weight_decay': 9.756837399842312e-05, 'max_epochs': 41} #best par of the new model
  print('best_params: ', best_params)
else:
  print('NUMBER_TRIALS: ', NUMBER_TRIALS)
print('USE_BEST_PARAMS: ', USE_BEST_PARAMS)


OPTIMIZE_FLAG = True
CALC_FI_FLAG = False
CALC_500m_FLAG = True
COMP_OCO2_FLAG = False
COMP_OCO3_FLAG = False

#set DC_CORRECT_SRSIF flag dependent on if model is trained on dc corrected SIF or not
# DC_CORRECT_SRSIF = False: SIFnet SIF is not dc corrected in comp2oco again
# DC_CORRECT_SRSIF = False: SIFnet SIF is dc corrected in comp2oco 
if DATA_OPTION == 19 or DATA_OPTION == 20:
  DC_CORRECT_SRSIF = False
else:
  DC_CORRECT_SRSIF = True

########################################################################################################################################################################
# define training area
d_start_train=1
d_end_train=69

#format: xLims1, yLims1;
#       xLims2, yLims2;
#       ...

Lims = [np.array([5, 20]),np.array([40, 55])]

#[np.array([0, 10]),np.array([40, 50])]

#[np.array([-10, 50]),np.array([25, 70])], #Europe
#[np.array([50, 110]),np.array([15, 60])] #Asia1
#[np.array([110, 170]),np.array([25, 70])] #Asia2
#[np.array([10, 70]),np.array([-45, 0])] #Africa
#[np.array([-90, -30]),np.array([-60, -15])], #South America
#[np.array([110, 170]),np.array([-45, 0])], #Australia
# [np.array([-130, -70]),np.array([15, 60])], #North America

validation_split = .1

#define test area
d_start_test=d_start_train
d_end_test=d_end_train
xLims_test = np.array([10, 15])
yLims_test = np.array([45, 50])


########################################################
#settings 500m super resolution
d_start_500m=1
d_end_500m=69

########################################################
#setting oco2 comparison
xLims_oco2Comp = xLims_test
yLims_oco2Comp = yLims_test
scalingOCO2Comp = 10 

########################################################
PRESCALE = False #should the data be upscaled to another grid as ground truth?
PRESCALE_FACTOR = -1
USE_MODEL_OUT = False #use model output from previous run if e.g. upscaled.
RESAMPLE_SIF = True #if SIF should be resampled to LR SIF in dataset class (False if SIF is no feature)

########################################################################################################################################################################
print('#####################################')
print('BATCH_SIZE: ', BATCH_SIZE)
print('DATA_OPTION: ', DATA_OPTION)
print('STEPS_TIME: ', STEPS_TIME)
print('STEPS_TIME_VAL: ', STEPS_TIME_VAL)
print('TRAIN_DATA_AUG_FLAG: ', TRAIN_DATA_AUG_FLAG)
print('SAMPLING_SIDE_LEN: ', SAMPLING_SIDE_LEN)
print('VAL_TEST_DATA_AUG_FLAG: ', VAL_TEST_DATA_AUG_FLAG)
print('SAMPLING_SIDE_LEN_VAL: ', SAMPLING_SIDE_LEN_VAL)
print('CV_FLAG: ', CV_FLAG)
print('MIN_EPOCHS: ', MIN_EPOCHS)
print('MAX_EPOCHS: ', EPOCHS)
print('ENQUEUE_TRIAL: ', ENQUEUE_TRIAL)
print('SCALE_FACTOR: ', SCALE_FACTOR)
print('FI_EPOCH: ', FI_EPOCH)
print('CALC_TESTLOSS_EPOCH: ', CALC_TESTLOSS_EPOCH)
print('#####################################')
print('OPTIMIZE_FLAG: ', OPTIMIZE_FLAG)
print('CALC_FI_FLAG: ', CALC_FI_FLAG)
print('CALC_500m_FLAG: ', CALC_500m_FLAG)
print('COMP_OCO2_FLAG: ', COMP_OCO2_FLAG)
print('COMP_OCO3_FLAG: ', COMP_OCO3_FLAG)
print('#####################################')
print('Lims: ', Lims)
print('Validation split: ', validation_split)
print('xLims_test: ', xLims_test)
print('yLims_test: ', yLims_test)
print('d_start_train: ', d_start_train)
print('d_end_train: ', d_end_train)
print('d_start_500m: ', d_start_500m)
print('d_end_500m: ', d_end_500m)
print('#####################################')
########################################################################################################################################################################

#create results folder
try:
  os.system('mkdir ' + pathPref + results_folder)
except:
  print('Save Folder already exists.') 
  


# Loss function L2 norm
mse_loss = nn.MSELoss()
def Calc_Loss(pred, target):
  #pred = torch.nan_to_num(pred, nan=0, posinf=0, neginf=0)
  if torch.nanmean(target).float() > 1:
    loss = mse_loss(torch.nanmean(pred).float(), torch.nanmean(target).float())*10
  elif torch.nanmean(target).float() > 0.7:
    loss = mse_loss(torch.nanmean(pred).float(), torch.nanmean(target).float())*6.7 
  elif torch.nanmean(target).float() > 0.4:
    loss = mse_loss(torch.nanmean(pred).float(), torch.nanmean(target).float())*3.3
  elif torch.nanmean(target).float() < 0:
    loss = mse_loss(torch.nanmean(pred).float(), torch.nanmean(target).float())*20  
  else:
    loss = mse_loss(torch.nanmean(pred).float(), torch.nanmean(target).float())
  return loss #mse_loss(torch.mean(pred[:,:5,:4]).float(), torch.mean(target[:,:5,:4]).float())
# train function
def train(model, optimizer, sample, device):
    model.train()
    optimizer.zero_grad()

    inp = sample['features'].to(device, dtype=torch.float)
    inp = inp.permute(0,3,1,2)
    target = sample['sif'].to(device)
    pred = model(inp)
    pred = pred.to(device)
    pred = pred.permute(0,2,3,1)
    pred = pred.squeeze(-1)

    loss = Calc_Loss(pred, target)
    loss.backward()
    optimizer.step()
    if loss==np.inf:
      print('torch.mean(pred).float(): ',torch.mean(pred).float())
      print('torch.mean(target).float(): ',torch.mean(target).float()) 
    #print('loss: ', loss)
    return loss

#validation function
def validate(model, sample, device):
    model.eval()
    
    with torch.no_grad():
        inp = sample['features'].to(device, dtype=torch.float)
        target = sample['sif'].to(device)
        inp = inp.permute(0,3,1,2)
        pred = model(inp)
        pred = pred.permute(0,2,3,1)
        pred = pred.squeeze(-1)
        loss = Calc_Loss(pred, target)
    return loss


def RunSet(model, dataloader, device):
  sample = next(iter(dataloader))['sif'].detach().numpy()
  pred = np.zeros((len(dataloader), sample.shape[2], sample.shape[3]))
  targ = np.zeros((len(dataloader), sample.shape[2], sample.shape[3]))

  model = model.to(device)

  i = 0
  sample = next(iter(dataloader))
  model.eval()
  for sample in dataloader:
    with torch.no_grad():
        inp = sample['features'].to(device, dtype=torch.float)
        target = sample['sif'].to(device)
        inp = inp.permute(0,3,1,2)
        pr = model(inp)
        pr = pr.permute(0,2,3,1)
        pr = pr.squeeze(-1)
        pred[i,::] = pr.cpu().detach().numpy()[0,0,::]
        targ[i,::] = target.cpu().detach().numpy()[0,0,::]
        i+=1

  return targ, pred

def load_checkpoint(filepath):
    if torch.cuda.is_available() == False or device == 'cpu':
        checkpoint = torch.load(filepath, map_location=torch.device('cpu')) 
    else:
        checkpoint = torch.load(filepath)
    model = checkpoint['model']
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)

    return model


################################################
# Load Data
################################################

if OPTIMIZE_FLAG or CALC_FI_FLAG or USE_BEST_PARAMS:
  dict_inp = {}
  dict_oco = {}

  dict_inp, keys, dict_oco, dates, ti_days_since = get_500m_dataset(xLims=Lims[0], yLims=Lims[1], xLims_test = np.array([10, 15]), yLims_test = np.array([45, 50]), d_start=d_start_train, d_end=d_end_train, USE_MODEL_OUT=USE_MODEL_OUT, SCALE_FACTOR=SCALE_FACTOR, DATA_OPTION=DATA_OPTION)

  inp_te, keys_te, dict_oco_te, dates_te, days_since_test = get_500m_dataset(xLims=xLims_test, yLims=yLims_test, yLims_test = np.array([200]), xLims_test = np.array([200]), d_start=d_start_test, d_end=d_end_test, USE_MODEL_OUT=USE_MODEL_OUT, SCALE_FACTOR=SCALE_FACTOR, DATA_OPTION=DATA_OPTION) #, RESAMPLE_SIF_FEAT=True

  print('Used features: ', keys)
  fig, ax = plt.subplots(figsize = (10,5))
  # Histogram:
  # Bin it
  n, bin_edges = np.histogram(dict_oco[:,0,0], 20)
  # Normalize it, so that every bins value gives the probability of that bin
  bin_probability = n/float(n.sum())
  # Get the mid points of every bin
  bin_middles = (bin_edges[1:]+bin_edges[:-1])/2.
  # Compute the bin-width
  bin_width = bin_edges[1]-bin_edges[0]
  # Plot the histogram as a bar plot
  plt.bar(bin_middles, bin_probability, width=bin_width)
  plt.xlabel('SIF $[mWm^{-2}sr^{-1}nm^{-1}]$')
  plt.ylabel('Probability')
  plt.grid(True)
  plt.xlim(0,2)
  plt.title('Training target')
  plt.show()
  plt.savefig(pathPref+results_folder+'Profile_of_training_target.pdf', format='pdf', bbox_inches = 'tight', pad_inches = 0)
  
  fig, ax = plt.subplots(figsize = (10,5))
  # Histogram:
  # Bin it
  n, bin_edges = np.histogram(dict_oco_te[:,0,0], 20)
  # Normalize it, so that every bins value gives the probability of that bin
  bin_probability = n/float(n.sum())
  # Get the mid points of every bin
  bin_middles = (bin_edges[1:]+bin_edges[:-1])/2.
  # Compute the bin-width
  bin_width = bin_edges[1]-bin_edges[0]
  # Plot the histogram as a bar plot
  plt.bar(bin_middles, bin_probability, width=bin_width)
  plt.xlabel('SIF $[mWm^{-2}sr^{-1}nm^{-1}]$')
  plt.ylabel('Probability')
  plt.grid(True)
  plt.xlim(0,2)
  plt.title('Testing target')
  plt.show()
  plt.savefig(pathPref+results_folder+'Profile_of_testing_target.pdf', format='pdf', bbox_inches = 'tight', pad_inches = 0)
  Tropo, Modis, Sen2 = helpers.GetGrids(xLims_test, yLims_test)
  lon,lat = Tropo['lon'],Tropo['lat']

  K_FOLDS = len(Lims)
  INPUT_CHANNELS = len(keys)
  print('INPUT_CHANNELS: ', INPUT_CHANNELS)
  d = dict_inp[0].shape[0]

  df_train = pd.DataFrame({},index=['lr', 'wd', 'loss','epoch'])
  df_test = pd.DataFrame({},index=['lr', 'wd', 'loss','epoch'])

  model = SR_SIF(INPUT_CHANNELS=INPUT_CHANNELS)
  print(model)

  #initialize test loss!
  #MIN_TEST_LOSS = np.inf
  df_MIN_TEST_LOSS = pd.DataFrame({'MIN_TEST_LOSS':[np.inf]})

  #create test dataset
  dataset_test = DataSet(inp_te, dict_oco_te) #
  dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE_TEST, shuffle=False)
  




################################################
# Train function
################################################
def train_evaluate(params, validation_split=validation_split, gpu_id=0, prod=False, test=False, FI_EPOCH=False, CALC_TESTLOSS_EPOCH=False):  #
  

  print('##############################################################################################################################')
  print('New trial with lr={}, wd={} and epochs={}, device={}'.format(params["learning_rate"], params["weight_decay"], params["max_epochs"], gpu_id))
  print('##############################################################################################################################')

  seed = 1
  np.random.seed(seed)
  torch.manual_seed(seed)
  if torch.cuda.is_available():
      torch.cuda.manual_seed_all(seed)
   
  #GPU device
  device = 'cuda:' +str(gpu_id)
  print('gpu_id: ', gpu_id)

  #arrays to save losses
  arr = np.zeros((EPOCHS,))
  arr[:] = np.nan
  df_losses = pd.DataFrame({}, index=np.arange(0,EPOCHS))
  df_losses['train'] = arr
  df_losses['val'] = arr
  df_losses['test'] = arr
  arr_train_loss = np.zeros((EPOCHS, K_FOLDS+1))
  arr_val_loss = np.zeros((EPOCHS, K_FOLDS+1))
  arr_test_loss = np.zeros((EPOCHS, K_FOLDS))
  arr_train_loss[:] = np.nan
  arr_val_loss[:] = np.nan
  arr_test_loss[:] = np.nan

  max_epoch = params["max_epochs"]

      
  #create training set and validation set
  dataset_tr = DataSet(dict_inp, dict_oco)
  dataset_size = len(dataset_tr)
  train_size = int((1-validation_split) * dataset_size)
  val_size = dataset_size - train_size
  dataset_tr, dataset_val = random_split(dataset_tr, [train_size, val_size])

  dataloader_tr = DataLoader(dataset_tr, batch_size=BATCH_SIZE, shuffle=True)
  dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE,shuffle=True)
  
  
  print('len(dataloader_tr):',len(dataloader_tr))
  print('len(dataloader_val):',len(dataloader_val))
  '''
  #create training dataset
  dataset_tr = DataSet(dict_inp, dict_oco)
  dataloader_tr = DataLoader(dataset_tr, batch_size=BATCH_SIZE, shuffle=True)
  '''
  #load model and optimizer
  model = SR_SIF(INPUT_CHANNELS=INPUT_CHANNELS)
  model = model.to(device)
  optimizer = optim.Adam(model.parameters(), lr=params["learning_rate"], weight_decay=params["weight_decay"])
  #scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)  #, cycle_momentum=False
  
  #train model
  for epoch in range(max_epoch):
      #training
      total_train_loss = 0.0
      train_rmse = 0.0
      train_r2 = 0.0
      train_ssim = 0.0
      i = 0

      for sample in dataloader_tr:

        curr_loss = train(model, optimizer, sample, device)
        
        i = i+1
        if curr_loss==np.inf:
          print('inf loss appears at training sampel ',i)
       
        total_train_loss += curr_loss

      total_train_loss = total_train_loss/round((len(dataset_tr)/BATCH_SIZE)+0.49)
      print('[EPOCH {}] TOTAL TRAIN LOSS (MSE): {}'.format(epoch, total_train_loss)) 

      arr_train_loss[epoch] = total_train_loss.cpu().detach().numpy()
      df_losses.loc[epoch,'train'] = total_train_loss.cpu().detach().numpy()
      
      #validation loss
      total_val_loss = 0.0
      for sample in dataloader_val:

        curr_loss = validate(model, sample, device)
        
        i = i+1
        if curr_loss==np.inf:
          print('inf loss appears at validation sampel ',i)
       
        total_val_loss += curr_loss

      total_val_loss = total_val_loss/round((len(dataset_val)/BATCH_SIZE)+0.49)
      print('[EPOCH {}] TOTAL VALIDATION LOSS (MSE): {}'.format(epoch, total_val_loss)) 

      arr_val_loss[epoch] = total_val_loss.cpu().detach().numpy()
      df_losses.loc[epoch,'val'] = total_val_loss.cpu().detach().numpy()
      

      


      #calculate Feature Importances every 5 epochs
      if FI_EPOCH:
        if epoch%5 == 0:
            FeatureImportance(inp_te, dict_oco_te, keys, dates_te, days_since_test, lon, lat, model, pathPref,results_folder, BATCH_SIZE_TEST, FI_RUNS=1, DATA_OPTION=DATA_OPTION, SaveFileName='Results_FeatureImportances_'+str(epoch)+'.nc')

      #calculate test loss every epoch
      if CALC_TESTLOSS_EPOCH:
        #run test set
        model.eval()
        with torch.no_grad():
            test_loss = 0.0
            
            loopSave=0
            for sample in dataloader_test:
                target = sample['sif'].to(device)
                inp = sample['features'].to(device, dtype=torch.float)
                inp = inp.permute(0,3,1,2)
                pred = model(inp)
                pred = pred.permute(0,2,3,1)
                pred = pred.squeeze(-1)
                loss = Calc_Loss(pred, target)
                test_loss += loss

        test_loss = test_loss/round((len(dataset_test)/BATCH_SIZE_TEST)+0.49)
  
        print('TOTAL TEST LOSS (MSE): {}'.format(test_loss)) 
        df_losses.loc[epoch,'test'] = test_loss.cpu().detach().numpy()
  
  if test or prod:
    sample = next(iter(dataloader_test))['sif'].detach().numpy()
    pred_mat = np.zeros((len(dataloader_test), sample.shape[1], sample.shape[2]))
    targ_mat = np.zeros((len(dataloader_test), sample.shape[1], sample.shape[2]))
    
  #run test set
  model.eval()
  with torch.no_grad():
      test_loss = 0.0
      loopSave=0
      for sample in dataloader_test:  
          target = sample['sif'].to(device)
          inp = sample['features'].to(device, dtype=torch.float)
          inp = inp.permute(0,3,1,2)
          pred = model(inp)
          pred = pred.permute(0,2,3,1)
          pred = pred.squeeze(-1)
          loss = Calc_Loss(pred, target)
          test_loss += loss

          if test or prod:
            pred_mat[loopSave,::] = pred.cpu().detach().numpy()[0,0,::]
            targ_mat[loopSave,::] = target.cpu().detach().numpy()[0,0,::]
            loopSave += 1

  test_loss = test_loss/round((len(dataset_test)/BATCH_SIZE_TEST)+0.49)

  print('TOTAL TEST LOSS (MSE): {}'.format(test_loss)) 

  #save losses
  df_train[str(params["learning_rate"])] = [params["learning_rate"], params["weight_decay"], total_train_loss.cpu().detach().numpy(), params["max_epochs"]]
  df_test[str(params["learning_rate"])] = [params["learning_rate"], params["weight_decay"], test_loss.cpu().detach().numpy(), params["max_epochs"]]
  
  
  MIN_TEST_LOSS = df_MIN_TEST_LOSS.MIN_TEST_LOSS.to_numpy()[0]
  if test_loss.cpu().detach().numpy() < df_MIN_TEST_LOSS.MIN_TEST_LOSS.to_numpy()[0]:
    print('Saving model.')
    df_MIN_TEST_LOSS.MIN_TEST_LOSS.to_numpy()[0] = test_loss.cpu().detach().numpy()
    checkpoint = {
            'model' : SR_SIF(INPUT_CHANNELS=INPUT_CHANNELS),
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            }
    torch.save(checkpoint, pathPref+results_folder+modelName_woTestSet)

  if prod:
    return test_loss, model, optimizer, df_losses, pred_mat, targ_mat
  else:
    return test_loss

N_GPUS = torch.cuda.device_count()

class GpuQueue:

    def __init__(self):
        self.queue = multiprocessing.Manager().Queue()
        all_idxs = list(range(N_GPUS)) if N_GPUS > 0 else [None]
        for idx in all_idxs:
            self.queue.put(idx)

    @contextmanager
    def one_gpu_per_process(self):
        current_idx = self.queue.get()
        yield current_idx
        self.queue.put(current_idx)


class Objective:

    def __init__(self, gpu_queue: GpuQueue):
        self.gpu_queue = gpu_queue

    def __call__(self, trial):
        params = {'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2),
              'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-3),
              'max_epochs': trial.suggest_int('max_epochs', MIN_EPOCHS, EPOCHS)}

        with self.gpu_queue.one_gpu_per_process() as gpu_i:
            print(gpu_i)
            return train_evaluate(params, gpu_id=gpu_i)
            

if USE_BEST_PARAMS == False and OPTIMIZE_FLAG == True:
  study = optuna.create_study(direction='minimize')#,pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource=EPOCHS, reduction_factor=3))
  if ENQUEUE_TRIAL:
    if DATA_OPTION == 1 and MOD==5:
      study.enqueue_trial({'learning_rate': 0.0000341467159499725, 'weight_decay': 9.756837399842312e-05, 'max_epochs': 41}) #DO12, MOD5
    elif DATA_OPTION == 1 and MOD==2:
      study.enqueue_trial({'learning_rate': 0.00011314985985718366, 'weight_decay': 8.001878825063443e-05, 'max_epochs': 36}) #DO12, MOD2
    elif DATA_OPTION == 1 and MOD==5 and SCALE_FACTOR == 50:
      study.enqueue_trial({'learning_rate': 0.013267189532114133, 'weight_decay': 8.672253673584394e-05, 'max_epochs': 28}) #DO12, MOD5, scaling 50
    elif DATA_OPTION == 16 and MOD==5:
      study.enqueue_trial({'learning_rate': 0.010675328620994972, 'weight_decay': 1.9057444091756266e-05, 'max_epochs': 32}) #DO16, MOD5

  study.optimize(Objective(GpuQueue()), n_trials=NUMBER_TRIALS, n_jobs=torch.cuda.device_count())
  best_params = study.best_params
  print('Best parameters: ', best_params)
  print('Best Test loss: ', study.best_value)

  print('df_train',df_train)
  print('df_test',df_test)
  df_train.to_csv(pathPref+results_folder+'/Optimization_TotalTrainLoss.csv')
  df_test.to_csv(pathPref+results_folder+'/Optimization_TotalTestLoss.csv')
  

elif USE_BEST_PARAMS:
  test_loss, model, optimizer, df_losses, pred_mat, targ_mat = train_evaluate(best_params, validation_split=validation_split, prod=True, FI_EPOCH=FI_EPOCH, CALC_TESTLOSS_EPOCH=CALC_TESTLOSS_EPOCH)
  df_losses.to_csv(pathPref+results_folder+'/TrainValLossEpochs.csv')

########################################################################################################
#Run with optimal parameters. Train without test set.
########################################################################################################
print('########################################################################################################')
print('Calculate test set with optimized model that was saved!')


model_woTestSet = load_checkpoint(pathPref+results_folder+modelName_woTestSet)
model_woTestSet = model_woTestSet.to(device)
'''
if OPTIMIZE_FLAG or USE_BEST_PARAMS:
  # do some plots
  dataset_test = DataSet(feat_te, sif_te, CV_FLAG_MULTIREGION=False, TrainFlag=VAL_TEST_DATA_AUG_FLAG, side_len=SAMPLING_SIDE_LEN_VAL, SCALE_FACTOR=SCALE_FACTOR, USE_MODEL_OUT=USE_MODEL_OUT, RESAMPLE_SIF = RESAMPLE_SIF) #
  dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE_TEST, shuffle=False)

  # run test set
  targ, pred = RunSet(model_woTestSet, dataloader_test, device='cuda:0')


  # do some plots
  helpers.Plot2dHistogram(X=targ.flatten(), Y=pred.flatten(), bins=(500,500), title_='Test data', y_label_='SIF$_{pred}$', x_label_='SIF', pathSave=pathPref+results_folder+'/HistResultTest_scaling'+str(SCALE_FACTOR)+'_woTestSet.pdf',xlim_=[-1.5,4.2],ylim_=[-1.5,4.2])

  fs = (10,10)
  plt.figure(figsize=fs)
  im = plt.imshow(np.nanmean(targ,axis=0), cmap='Greens', vmin=0, vmax=2.5, extent=[lon[0], lon[-1], lat[0], lat[-1]])
  cbar = plt.colorbar(im)
  cbar.set_label('SIF@740nm')
  plt.xlabel('longitude')
  plt.ylabel('latitude')
  plt.title('Ground Truth SIF, Test Data, 04/2018-03/2019, 0.05°')
  plt.savefig(pathPref+results_folder+'/GT_SIF_woTestSet.pdf', bbox_inches = 'tight', pad_inches = 0)

  ssim_ = ssim(np.nanmean(targ,axis=0), np.nanmean(pred,axis=0))
  plt.figure(figsize=fs)
  im = plt.imshow(np.nanmean(pred,axis=0), cmap='Greens', vmin=0, vmax=2.5, extent=[lon[0], lon[-1], lat[0], lat[-1]])
  cbar = plt.colorbar(im)
  cbar.set_label('SIF@740nm')
  plt.xlabel('longitude')
  plt.ylabel('latitude')
  plt.title('Estimated SIF, Test Data, SSIM='+str(np.round(ssim_,2)))
  plt.savefig(pathPref+results_folder+'/PredSIF_woTestSet.pdf', bbox_inches = 'tight', pad_inches = 0)

  plt.figure(figsize=fs)
  im = plt.imshow(np.nanmean(targ-pred,axis=0), cmap='seismic', vmin=-1, vmax=1, extent=[lon[0], lon[-1], lat[0], lat[-1]])
  cbar = plt.colorbar(im)
  cbar.set_label('SIF@740nm')
  plt.xlabel('longitude')
  plt.ylabel('latitude')
  plt.title('SIF, difference targ-pred, Test Data, SSIM='+str(np.round(ssim_,2)))
  plt.savefig(pathPref+results_folder+'/DiffTargPredSIF_woTestSet.pdf', bbox_inches = 'tight', pad_inches = 0)

  print('SSIM = ', np.round(ssim_,2))

  OutputFile = netCDF4.Dataset(pathPref+results_folder+'/ResultTestSet_scaling'+str(SCALE_FACTOR)+'_woTestSet.nc', "w", format="NETCDF4")

  len1 = OutputFile.createDimension("lon", pred.shape[2])
  len2 = OutputFile.createDimension("lat", pred.shape[1])
  len3 = OutputFile.createDimension("time", pred.shape[0])

  var_lon = OutputFile.createVariable("lon", "f4", ("lon",))
  var_lat = OutputFile.createVariable("lat", "f4", ("lat",))
  var_dates = OutputFile.createVariable("time", "f4", ("time",))
  var_sif_pred = OutputFile.createVariable("sif_pred", "f4", ("time","lat","lon"))
  var_sif_gt = OutputFile.createVariable("sif_gt", "f4", ("time","lat","lon"))

  var_dates.units = 'days since 1970-01-01'
  var_lon[:] = lon
  var_lat[:] = lat
  var_sif_pred[:] = pred
  var_sif_gt[:] = targ
  var_dates[:] = days_since_test

  OutputFile.close()

if OPTIMIZE_FLAG or CALC_FI_FLAG:
  del dict_feat, dict_sif
'''
########################################################################################################
#Do further operations.
########################################################################################################

########## Calculate Feature importances
if CALC_FI_FLAG:
  FeatureImportance(inp_te, dict_oco_te, keys, dates_te, days_since_test, lon, lat, model_woTestSet, pathPref,results_folder, BATCH_SIZE_TEST, FI_RUNS=1, DATA_OPTION=DATA_OPTION, SaveFileName='Results_FeatureImportances.nc')
  print('Calculate Feature Importances done!')

########## Calculate 500m result
if CALC_500m_FLAG:
  Calc500m(model_woTestSet, DATA_OPTION, pathPref, SCALE_FACTOR, results_folder,d_start_500m,d_end_500m)
  print('Calculate 500m output done!')

########## Compare to OCO-2 data
if COMP_OCO2_FLAG:
  X = 2
  Comp2OCO2(PathLoad_OCO2='/mnt/mnt_folder/data/', PathLoad_Down='/mnt/mnt_folder/data/', pathLoad_SR=pathPref+results_folder, pathSave=pathPref+results_folder+'results_OCO'+str(X)+'/', xLims=xLims_oco2Comp, yLims=yLims_oco2Comp, scaling=scalingOCO2Comp, X=X, DC_CORRECT_SRSIF=DC_CORRECT_SRSIF)
  print('Compare to OCO-2 data done!')

if COMP_OCO3_FLAG:
  X = 3
  Comp2OCO2(PathLoad_OCO2='/mnt/mnt_folder/data/', PathLoad_Down='/mnt/mnt_folder/data/', pathLoad_SR=pathPref+results_folder, pathSave=pathPref+results_folder+'results_OCO'+str(X)+'/', xLims=xLims_oco2Comp, yLims=yLims_oco2Comp, scaling=scalingOCO2Comp, X=X, DC_CORRECT_SRSIF=DC_CORRECT_SRSIF)
  print('Compare to OCO-3 data done!')